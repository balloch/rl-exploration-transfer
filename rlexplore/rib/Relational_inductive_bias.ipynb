{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDJ4mkET9WPL"
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3\n",
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kk5j2zesDemW"
      },
      "outputs": [],
      "source": [
        "!pip install 'shimmy>=0.2.1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "eF3RqM0r9E8C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "class Relational(nn.Module):\n",
        "    def __init__(self, input_shape, nheads=1, hidden_dim=None, output_dim=None):\n",
        "        super(Relational, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.nheads = nheads\n",
        "        self.features = input_shape[-1]\n",
        "        if hidden_dim is None:\n",
        "            self.hidden_dim = self.features\n",
        "        else:\n",
        "            self.hidden_dim = hidden_dim\n",
        "        if output_dim is None:\n",
        "            self.output_dim = self.features\n",
        "        else:\n",
        "            self.output_dim = output_dim\n",
        "\n",
        "        self.q_projection = nn.Linear(self.features, self.hidden_dim)\n",
        "        self.k_projection = nn.Linear(self.features, self.hidden_dim)\n",
        "        self.v_projection = nn.Linear(self.features, self.hidden_dim)\n",
        "        self.output_linear = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._apply_self_attention(x)\n",
        "        x = self.output_linear(x)\n",
        "        return x\n",
        "\n",
        "    def _apply_self_attention(self, x):\n",
        "        q = self.q_projection(x)\n",
        "        k = self.k_projection(x)\n",
        "        v = self.v_projection(x)\n",
        "\n",
        "        q = q.view(*q.shape[:-1], self.nheads, -1).transpose(-2, -3).to(device)\n",
        "        k = k.view(*k.shape[:-1], self.nheads, -1).transpose(-2, -3).to(device)\n",
        "        v = v.view(*v.shape[:-1], self.nheads, -1).transpose(-2, -3).to(device)\n",
        "\n",
        "        d = torch.tensor([self.features], dtype=x.dtype).to(device)\n",
        "        w = F.softmax(torch.matmul(q, k.transpose(-1, -2)) / torch.sqrt(d), dim=-1).to(device)\n",
        "        # print(w.device)\n",
        "        # print(v.device)\n",
        "        scores = torch.matmul(w, v)\n",
        "\n",
        "        scores = scores.transpose(-2, -3)\n",
        "        scores = scores.view(*scores.shape[:-2], -1)\n",
        "\n",
        "        return scores\n",
        "\n",
        "class RelationalActorCritic(nn.Module):\n",
        "    def __init__(self, obs_shape, a_dim, lin_dims, relational_hidden_dim=None, relational_output_dim=None):\n",
        "        super(RelationalActorCritic, self).__init__()\n",
        "        self.obs_shape = obs_shape\n",
        "        self.a_dim = a_dim\n",
        "\n",
        "        self.relational = Relational(\n",
        "            obs_shape,\n",
        "            hidden_dim=relational_hidden_dim,\n",
        "            output_dim=relational_output_dim,\n",
        "        )\n",
        "\n",
        "        lin_dims.insert(0, obs_shape[0])\n",
        "        lin_dims.append(a_dim)\n",
        "        lin_module_list = []\n",
        "        for i in range(len(lin_dims) - 1):\n",
        "            lin_module_list.append(nn.Linear(lin_dims[i], lin_dims[i + 1]))\n",
        "            lin_module_list.append(nn.ReLU())\n",
        "        self.linear = nn.Sequential(*lin_module_list)\n",
        "        self.policy_head = nn.Linear(a_dim, a_dim)\n",
        "        self.baseline_head = nn.Linear(a_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relational(x)\n",
        "        # print(\"Before max:\", x.shape)\n",
        "        x = torch.max(x, dim=-2, keepdim=True).values\n",
        "        # print(\"After max:\", x.shape)\n",
        "        x = self.linear(x)\n",
        "        # print(\"After linear:\", x.shape)\n",
        "        b = self.baseline_head(x)\n",
        "        pi_logits = self.policy_head(x)\n",
        "        # print(\"Shape of pi_logits:\", pi_logits.shape)\n",
        "        return pi_logits, b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "AxwG9Y-i9IsW"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "import gym\n",
        "\n",
        "# Define a custom features extractor using the relational architecture\n",
        "class RelationalExtractor(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: gym.spaces.Space, **kwargs):\n",
        "        super(RelationalExtractor, self).__init__(observation_space, features_dim=128)\n",
        "\n",
        "        self.net = RelationalActorCritic(\n",
        "            obs_shape=observation_space.shape,\n",
        "            a_dim=128,\n",
        "            lin_dims=[128]\n",
        "        )\n",
        "\n",
        "    # def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
        "    #     _, features = self.net(observations)\n",
        "    #     return features\n",
        "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
        "      features, _ = self.net(observations)\n",
        "      return features\n",
        "\n",
        "# Create a custom actor-critic policy\n",
        "class CustomActorCritic(ActorCriticPolicy):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(CustomActorCritic, self).__init__(*args, **kwargs)\n",
        "\n",
        "# Now, use this custom policy with PPO\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=RelationalExtractor,\n",
        "    net_arch=[128]  # This can be changed according to your needs.\n",
        ")\n",
        "\n",
        "# model = PPO(CustomActorCritic, \"MountainCar-v0\", verbose=1, policy_kwargs=policy_kwargs)\n",
        "# model.learn(total_timesteps=10000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz_wdPsNQFh2"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "# Create a separate evaluation environment\n",
        "eval_env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# Create the callback\n",
        "# eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/best_model',\n",
        "#                              log_path='./logs/results', eval_freq=1000)\n",
        "\n",
        "model = PPO(CustomActorCritic, \"MountainCar-v0\", verbose=1, policy_kwargs=policy_kwargs)\n",
        "# model.learn(total_timesteps=1000000, callback=eval_callback)\n",
        "\n",
        "model.learn(total_timesteps=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "lfVt9b-f-A9i"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import numpy as np\n",
        "\n",
        "class CustomEvalCallback(BaseCallback):\n",
        "    def __init__(self, eval_env, eval_freq=1000, log_path='./logs/results', verbose=1):\n",
        "        super(CustomEvalCallback, self).__init__(verbose)\n",
        "        self.eval_env = eval_env\n",
        "        self.eval_freq = eval_freq\n",
        "        self.log_path = log_path\n",
        "        self.best_mean_reward = -np.inf\n",
        "        self.losses = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            mean_reward, std_reward = self.evaluate_policy()\n",
        "            self.logger.record(\"eval/mean_reward\", mean_reward)\n",
        "            self.logger.record(\"eval/std_reward\", std_reward)\n",
        "            if mean_reward > self.best_mean_reward:\n",
        "                self.best_mean_reward = mean_reward\n",
        "                self.logger.info(f\"New best mean reward: {mean_reward}! Saving model to {self.log_path}/best_model.zip\")\n",
        "                self.model.save(f\"{self.log_path}/best_model\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def evaluate_policy(self, num_episodes=10):\n",
        "        all_rewards = []\n",
        "        for _ in range(num_episodes):\n",
        "            obs = self.eval_env.reset()\n",
        "            done = False\n",
        "            episode_reward = 0\n",
        "            while not done:\n",
        "                action, _states = self.model.predict(obs, deterministic=True)\n",
        "                obs, reward, done, _ = self.eval_env.step(action)\n",
        "                episode_reward += reward\n",
        "            all_rewards.append(episode_reward)\n",
        "\n",
        "        mean_reward = np.mean(all_rewards)\n",
        "        std_reward = np.std(all_rewards)\n",
        "\n",
        "        return mean_reward, std_reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbZost8dOZ17"
      },
      "outputs": [],
      "source": [
        "eval_env = gym.make(\"MountainCar-v0\")\n",
        "custom_eval_callback = CustomEvalCallback(eval_env, eval_freq=1000, log_path='./logs/results')\n",
        "\n",
        "model = PPO(CustomActorCritic, \"MountainCar-v0\", verbose=1, policy_kwargs=policy_kwargs)\n",
        "model.learn(total_timesteps=1000000, callback=custom_eval_callback)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
